{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**What is LLMs surprisal and how it is computed?**\n","\n","* Surprisal in a language model is a metric of difficulty in understanding the language with an unexpected or a surprisal of a word based on its previous context.\n","* LMs provide a probability distribution over possible next words\n","* The input text is divided into tokens ans each token is converted into a vector representation that captures its meaning in the context on the surrounding words.\n","* Output of a neural network(logits) are the raw predictions of the next token in the given preceeding context and these raw predictions are passed through an activation function called softmax which calculates the predictions into probabilities.\n","* The probability is then used to compute the surprisal valur)negative logaritham base 2) for that word"],"metadata":{"id":"lVIgdZ32ld3s"}},{"cell_type":"markdown","source":["###Understanding LLMs using the resources\n","\n","####1. Psychformers.py:\n","* It handles both causal and masked language models,\n","* For each model, it loads the corresponding tokenizer and model architecture.\n","* In causal models, the model computes the probability of each target word given the preceding context.\n","* In masked models, the model computes the probability of each target word given the surrounding context, which includes both preceding and following words.\n","* In causal masked models, the model computes the probability of each target word given the surrounding context, similar to masked model processing but with considerations for causal modeling.\n","\n","####2. Surprisal.py:\n","* Models experimented: GPT-2, GPTneo, GPT3, KenLM(N-gram based language model)\n","* 2 main classes : HuggingFaceSurprisal and NgramSurprisal\n","* HuggingFaceSurprisal:\n","  * It takes list of tokens and an array of surprisal values\n","  * Returns the total surprise for a piece of text by indexing it\n","\n","* NgramSurprisal:\n","  * total surprise based on whole words, not individual characters\n","\n","* Using helper function it processes\n","  * Character-based slice: You select a range of characters from the original text.\n","  * Word-based slice: You select tokens by their word positions."],"metadata":{"id":"TtszbDzbn4Ro"}},{"cell_type":"markdown","source":["###Computing surprisal using GPT2(Causal Model)"],"metadata":{"id":"2N1RxyF8i-NQ"}},{"cell_type":"code","execution_count":21,"metadata":{"id":"kVSvzmhxfm2b","executionInfo":{"status":"ok","timestamp":1739385164887,"user_tz":300,"elapsed":20590,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}}},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM\n","import csv, math\n","import torch.nn.functional as F\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","# Set the model name\n","causal_model_name = \"openai-community/gpt2-large\"\n","\n","# Load the model and tokenizer\n","causal_tokenizer = AutoTokenizer.from_pretrained(causal_model_name)\n","causal_model = AutoModelForCausalLM.from_pretrained(causal_model_name)\n","\n","# Set the model to evaluation mode\n","causal_model.eval()\n","\n","def calculate_surprisal_causal(sentence):\n","    inputs = causal_tokenizer(sentence, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        outputs = causal_model(**inputs, labels=inputs[\"input_ids\"])\n","\n","    # Get token log probabilities\n","    log_probs = outputs.logits.log_softmax(dim=-1)\n","    input_ids = inputs[\"input_ids\"][0]\n","\n","    causal_surprisals = []\n","    for i in range(0, len(input_ids)):\n","      token_id = input_ids[i]\n","      if i == 0:\n","          log_prob = log_probs[0, i, token_id].item()\n","          surprisal = -log_prob\n","      else:\n","          prev_log_prob = log_probs[0, i - 1, token_id].item()\n","          surprisal = -prev_log_prob\n","      causal_surprisals.append((causal_tokenizer.decode([token_id]), surprisal))\n","\n","    surprisal_causal_df = pd.DataFrame(causal_surprisals, columns=[\"token\", \"surprisal\"])\n","    #return causal_surprisals\n","    return surprisal_causal_df"]},{"cell_type":"markdown","source":["###Computing surprisal for BERT (masked model)\n","\n","#### How BERT model works? resource: [Research Paper](https://arxiv.org/pdf/1810.04805)\n","\n","* Fine-Tuning and feature based approach.\n","* Fine-tuning approach involves adding a simple classification layer to the pre-trained model and all parameters are jointly fine-tuned on a downstream task.\n","* The fine-tuning approach, such as the Generative Pre-trained Transformer , introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pre-trained parameters.\n","* For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the\n","downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters.\n","* Feature based approach allows you to levarage the knowledge embedded in a pre-trained model by using it to create fixed representations of the input data without modyfing the model itself.\n","* After you have these embeddings, you take them as input to a simple, often lighter model like a logistic regression classifier or a basic neural network, which is specifically designed for your task (e.g., sentiment analysis, named entity recognition, etc.).\n","* This new model (which we sometimes call a classification layer) does not change the parameters of the pre-trained BERT model; it only learns from the fixed representations produced by BERT.\n","* BERT learns not only to fill in the masked words but also to understand relationships between pairs of sentences.\n","* The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features.\n","\n","* Task #1: Mask Language Modeling\n","    * we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a “masked\n","    LM” (MLM)(cloze task)\n","    * Although this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and\n","    fine-tuning, since the [MASK] token does not appear during fine-tuning. To mitigate this, we do not always replace “masked” words with the ac-\n","    tual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace\n","    the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, it will be used to predict the original token with cross entropy loss.\n","\n","* Task #2: Next Sentence Prediction (NSP):\n","  * when choosing the sentences A and B for each pre-\n","  training example, 50% of the time B is the actual\n","  next sentence that follows A (labeled as IsNext),\n","  and 50% of the time it is a random sentence from\n","  the corpus (labeled as NotNext).\n"],"metadata":{"id":"yVCIbQtwrrM-"}},{"cell_type":"markdown","source":["###Computing surprisal using Mask Model"],"metadata":{"id":"cbRIq91VmLEG"}},{"cell_type":"code","source":["masked_model_name =\"bert-base-uncased\"\n","\n","masked_tokenizer = AutoTokenizer.from_pretrained(masked_model_name)\n","masked_model = AutoModelForMaskedLM.from_pretrained(masked_model_name)\n","\n","masked_model.eval()\n","\n","def calculate_surprisal_masked(sentence):\n","    inputs = masked_tokenizer(sentence, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        outputs = masked_model(**inputs, labels=inputs[\"input_ids\"])\n","\n","     # Get token log probabilities\n","    log_probs = outputs.logits.log_softmax(dim=-1)\n","    input_ids = inputs[\"input_ids\"][0]\n","\n","    masked_surprisals = []\n","\n","    for i in range(0, len(input_ids)):\n","        # Calculate surprisal for each token as -log(p)\n","        token_id = input_ids[i]\n","        if i == 0:\n","            # For the first token ([CLS]), there is no previous context;\n","            # we use its own position's log probability.\n","            log_prob = log_probs[0, i, token_id].item()\n","            surprisal = -log_prob\n","        else:\n","          prev_log_prob = log_probs[0, i - 1, token_id].item()\n","          surprisal = -prev_log_prob\n","        masked_surprisals.append((masked_tokenizer.decode([token_id]), surprisal))\n","\n","\n","    surprisal_masked_df = pd.DataFrame(masked_surprisals, columns=[\"token\", \"surprisal\"])\n","    return surprisal_masked_df\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TFssC12XmaQY","executionInfo":{"status":"ok","timestamp":1739385165907,"user_tz":300,"elapsed":1022,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}},"outputId":"97be1a98-1d89-4488-e506-2bb58e543424"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"markdown","source":["###Computing surprisal using Permutation based model : XLNET"],"metadata":{"id":"aoQRF8jw-EIA"}},{"cell_type":"markdown","source":["#### How XLNet works?\n","* XLNet considers all possible word order permutations in a sequence during training.\n","* Captures bidirectional context without masking technique."],"metadata":{"id":"WHKIdBLH-INE"}},{"cell_type":"code","source":["from transformers import XLNetTokenizer, XLNetLMHeadModel\n","\n","permuation_model_name = \"xlnet-base-cased\"\n","permuation_tokenizer = XLNetTokenizer.from_pretrained(permuation_model_name)\n","permuation_model = XLNetLMHeadModel.from_pretrained(permuation_model_name)\n","permuation_model.eval()\n","\n","def calculate_surprisal_permutation(sentence):\n","    # Encode the sentence into input IDs\n","    input_ids = permuation_tokenizer.encode(sentence, return_tensors='pt')\n","\n","    with torch.no_grad():\n","        # Forward pass with labels so that the model computes logits for the entire sequence\n","        outputs = permuation_model(input_ids, labels=input_ids)\n","    logits = outputs.logits\n","\n","    # Shift the logits and labels to align each prediction with its corresponding token\n","    shifted_logits = logits[:, :-1, :]\n","    shifted_labels = input_ids[:, 1:]\n","\n","    # Convert logits to log probabilities over the vocabulary\n","    log_probs = torch.log_softmax(shifted_logits, dim=-1)\n","\n","    # For each position, extract the log probability assigned to the true token\n","    token_log_probs = log_probs.gather(2, shifted_labels.unsqueeze(-1)).squeeze(-1)\n","\n","    # Compute surprisal as the negative log probability (apply unary minus to tensor before converting to list)\n","    computed_surprisals = (-token_log_probs.squeeze()).tolist()\n","\n","    # Convert the full input_ids (without slicing) to tokens.\n","    tokens = permuation_tokenizer.convert_ids_to_tokens(input_ids[0])\n","\n","    # Prepend a placeholder (None) for the first token since its surprisal isn't computed (no left context)\n","    full_surprisals = [None] + computed_surprisals\n","\n","    return tokens, full_surprisals"],"metadata":{"id":"7KLpSLUW92uJ","executionInfo":{"status":"ok","timestamp":1739385169814,"user_tz":300,"elapsed":3909,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["###1. Analyzing sentences"],"metadata":{"id":"wCIE43cum_i9"}},{"cell_type":"code","source":["sentences = {\n","    # 'sentence_1': \"I ate a pizza yesterday.\",  # correct sentence\n","    # 'sentence_2': \"I ate a book yesterday.\",     # semantic violation\n","    # 'sentence_3': \"I eat a pizza yesterday.\",      # morphological error\n","    # 'sentence_4': \"I ate a piza yesterday.\",       # lexical error\n","    'sentence_5': \"The horse raced past the barn fell.\", #garden path sentence\n","    'sentence_6': \"the horse which was raced past the barn fell\" #garden path sentence\n","}"],"metadata":{"id":"9ErSwQlip9oz","executionInfo":{"status":"ok","timestamp":1739385453831,"user_tz":300,"elapsed":88,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}}},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":["### 1.1 Causal Model:"],"metadata":{"id":"0VclbEMy8J-R"}},{"cell_type":"code","source":["surprisal_causal_results = []\n","\n","for sentence_type, sentence in sentences.items():\n","    df = calculate_surprisal_causal(sentence)\n","    df['sentence'] = sentence\n","    print(df)\n","    print(\"*\" * 50)\n","    surprisal_causal_results.append(df)\n","\n","surprisal_causal_df = pd.concat(surprisal_causal_results, ignore_index=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t05_aSQP8JaI","executionInfo":{"status":"ok","timestamp":1739385457263,"user_tz":300,"elapsed":2098,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}},"outputId":"939a3831-e3e8-4485-8537-60717d0812d6"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["    token  surprisal                             sentence\n","0     The  12.220047  The horse raced past the barn fell.\n","1   horse   8.996132  The horse raced past the barn fell.\n","2   raced   7.437644  The horse raced past the barn fell.\n","3    past   3.689770  The horse raced past the barn fell.\n","4     the   1.376319  The horse raced past the barn fell.\n","5    barn   5.620616  The horse raced past the barn fell.\n","6    fell  12.037618  The horse raced past the barn fell.\n","7       .   4.344603  The horse raced past the barn fell.\n","**************************************************\n","    token  surprisal                                      sentence\n","0     the   9.726631  the horse which was raced past the barn fell\n","1   horse   9.300182  the horse which was raced past the barn fell\n","2   which   6.401368  the horse which was raced past the barn fell\n","3     was   2.283421  the horse which was raced past the barn fell\n","4   raced   9.934964  the horse which was raced past the barn fell\n","5    past   5.856984  the horse which was raced past the barn fell\n","6     the   1.918164  the horse which was raced past the barn fell\n","7    barn   5.566125  the horse which was raced past the barn fell\n","8    fell   8.007463  the horse which was raced past the barn fell\n","**************************************************\n"]}]},{"cell_type":"markdown","source":["**Observations:**\n","\n","* Misspellings can force the tokenizer to break a word into unexpected sub-units, which in turn increases the surprisal because the sub-tokens are less likely in that context.\n","\n","* For garden path sentence, surprisal values capture the increased processing difficulty for parts of the sentence where the structure is ambiguous or requires reanalysis."],"metadata":{"id":"zGmpcYMXyXGW"}},{"cell_type":"markdown","source":["### 1.2 Masked Model:"],"metadata":{"id":"8xwtbhEF9JEi"}},{"cell_type":"code","source":["surprisal_masked_results = []\n","\n","for sentence_type, sentence in sentences.items():\n","    df = calculate_surprisal_masked(sentence)\n","    df['sentence'] = sentence\n","    print(df)\n","    print(\"*\" * 50)\n","    surprisal_masked_results.append(df)\n","\n","surprisal_masked_df = pd.concat(surprisal_masked_results, ignore_index=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jm-m01D5oaeD","executionInfo":{"status":"ok","timestamp":1739385469225,"user_tz":300,"elapsed":473,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}},"outputId":"cc2b7b63-1635-469c-c3d1-e84e5ef11b6f"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["   token  surprisal                             sentence\n","0  [CLS]  13.900969  The horse raced past the barn fell.\n","1    the   4.236541  The horse raced past the barn fell.\n","2  horse  13.359109  The horse raced past the barn fell.\n","3  raced  18.895639  The horse raced past the barn fell.\n","4   past  11.975198  The horse raced past the barn fell.\n","5    the  10.862679  The horse raced past the barn fell.\n","6   barn  17.551842  The horse raced past the barn fell.\n","7   fell  18.181185  The horse raced past the barn fell.\n","8      .   4.902893  The horse raced past the barn fell.\n","9  [SEP]  30.795389  The horse raced past the barn fell.\n","**************************************************\n","    token  surprisal                                      sentence\n","0   [CLS]  14.039317  the horse which was raced past the barn fell\n","1     the   4.346106  the horse which was raced past the barn fell\n","2   horse  17.891470  the horse which was raced past the barn fell\n","3   which  13.686981  the horse which was raced past the barn fell\n","4     was  17.728954  the horse which was raced past the barn fell\n","5   raced  21.644361  the horse which was raced past the barn fell\n","6    past  11.555975  the horse which was raced past the barn fell\n","7     the  12.847952  the horse which was raced past the barn fell\n","8    barn  21.577297  the horse which was raced past the barn fell\n","9    fell  17.571987  the horse which was raced past the barn fell\n","10  [SEP]  25.531538  the horse which was raced past the barn fell\n","**************************************************\n"]}]},{"cell_type":"markdown","source":["**Observations:**\n","\n","1. [CLS] -> high surprisal because it is a special token that lacks preceding context and it receives a lower probability estimate which results in high surprisal.\n","2. [SEP] is rare and appears only in specific, limited contexts, the model isn’t as good at predicting it compared to common words, which leads to a higher surprisal value when [SEP] appears."],"metadata":{"id":"CYQIquaO0DF1"}},{"cell_type":"markdown","source":["### 1.3 Permutation Model"],"metadata":{"id":"k5VVoEMY79ED"}},{"cell_type":"code","source":["surprisal_permutation_results = []\n","\n","for sentence_type, sentence in sentences.items():\n","    tokens, surprisals = calculate_surprisal_permutation(sentence)\n","    df = pd.DataFrame({'token': tokens, 'surprisal': surprisals})\n","    df['sentence'] = sentence\n","    print(df)\n","    print(\"*\" * 50)\n","    surprisal_permutation_results.append(df)\n","\n","surprisal_permutation_df = pd.concat(surprisal_permutation_results, ignore_index=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MQ1V9yEY-nHc","executionInfo":{"status":"ok","timestamp":1739385494901,"user_tz":300,"elapsed":527,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}},"outputId":"a179e3ff-1c1f-46fb-8e23-1bf051f39fdb"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["    token  surprisal                             sentence\n","0    ▁The        NaN  The horse raced past the barn fell.\n","1  ▁horse   9.267667  The horse raced past the barn fell.\n","2  ▁raced  10.447077  The horse raced past the barn fell.\n","3   ▁past   5.547226  The horse raced past the barn fell.\n","4    ▁the   9.646189  The horse raced past the barn fell.\n","5   ▁barn   8.861709  The horse raced past the barn fell.\n","6   ▁fell   8.939671  The horse raced past the barn fell.\n","7       .   6.742737  The horse raced past the barn fell.\n","8   <sep>  13.904520  The horse raced past the barn fell.\n","9   <cls>   9.846058  The horse raced past the barn fell.\n","**************************************************\n","     token  surprisal                                      sentence\n","0     ▁the        NaN  the horse which was raced past the barn fell\n","1   ▁horse   7.157487  the horse which was raced past the barn fell\n","2   ▁which   4.895946  the horse which was raced past the barn fell\n","3     ▁was   1.850880  the horse which was raced past the barn fell\n","4   ▁raced  13.541876  the horse which was raced past the barn fell\n","5    ▁past   3.768576  the horse which was raced past the barn fell\n","6     ▁the   6.869979  the horse which was raced past the barn fell\n","7    ▁barn   7.821211  the horse which was raced past the barn fell\n","8    ▁fell   4.445508  the horse which was raced past the barn fell\n","9    <sep>  18.459595  the horse which was raced past the barn fell\n","10   <cls>   6.732624  the horse which was raced past the barn fell\n","**************************************************\n"]}]},{"cell_type":"code","source":["print(permuation_tokenizer.convert_tokens_to_ids(\"▁ate\")) ## means _ate isn't in the library"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lR4cavt-DIoK","executionInfo":{"status":"ok","timestamp":1739385179734,"user_tz":300,"elapsed":6,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}},"outputId":"87e9442e-7ef1-4e18-d14e-7791a5162729"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]}]},{"cell_type":"markdown","source":["**Observations:**\n","\n","1. \"▁I\" = NaN as no probability is computed for the first token since it has no left context.\n","2. For Morphological error sentence(3), the probability for “yesterday,”, the model is not relying solely on the left context “I eat a pizza” but also on any available right context and on the overall likelihood of “yesterday” occurring in similar contexts."],"metadata":{"id":"i22-QkK1BCCr"}},{"cell_type":"markdown","source":["###2. Masking the target token"],"metadata":{"id":"D6Tq12Xk8-uA"}},{"cell_type":"code","source":["def analyze_masked_sentence(masked_text):\n","    inputs = masked_tokenizer(masked_text, return_tensors=\"pt\")\n","    mask_token_index = torch.where(inputs['input_ids'][0] == masked_tokenizer.mask_token_id)[0]\n","\n","    with torch.no_grad():\n","        outputs = masked_model(**inputs)\n","        predictions = outputs.logits[0, mask_token_index]\n","\n","    # Get top 5 predictions\n","    top_5 = torch.topk(predictions, 5, dim=1)\n","    probs = torch.softmax(top_5.values[0], dim=0)\n","\n","    predicted_tokens = masked_tokenizer.convert_ids_to_tokens(top_5.indices[0])\n","    surprisals = -torch.log(probs).numpy()\n","\n","    return pd.DataFrame({\n","        'predicted_token': predicted_tokens,\n","        'surprisal': surprisals\n","    })\n","\n","# Test masked prediction\n","masked_sentence = \"I ate a [MASK] yesterday.\"\n","mask_results = analyze_masked_sentence(masked_sentence)"],"metadata":{"id":"ArdXfIH7HMch","executionInfo":{"status":"ok","timestamp":1739385180096,"user_tz":300,"elapsed":220,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["print(\"\\nTop 5 predictions for masked token:\")\n","for _, row in mask_results.iterrows():\n","    print(f\"Token: {row['predicted_token']}, Surprisal: {row['surprisal']:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WDI3xI2-HmxI","executionInfo":{"status":"ok","timestamp":1739385180096,"user_tz":300,"elapsed":3,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}},"outputId":"83c2e912-2cc2-40f1-9f57-9bcb21d9629c"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Top 5 predictions for masked token:\n","Token: lot, Surprisal: 0.391\n","Token: little, Surprisal: 1.756\n","Token: sandwich, Surprisal: 2.324\n","Token: fish, Surprisal: 3.627\n","Token: few, Surprisal: 3.628\n"]}]},{"cell_type":"markdown","source":["### 3. Punctuations"],"metadata":{"id":"eYyYqCcI-htZ"}},{"cell_type":"code","source":["punct_sentences = [\n","    # \"I ate a pizza yesterday.\",\n","    # \"I ate a pizza, yesterday.\",\n","    # \"I ate a pizza yesterday; I enjoyed every bite.\",\n","    # \"I ate an apple? No way!\",\n","    \"The man hunted the deer ran into the woods.\",\n","    \"The man hunted, the deer ran into the woods.\"\n","]"],"metadata":{"id":"JjjbXLvLIklc","executionInfo":{"status":"ok","timestamp":1739385827603,"user_tz":300,"elapsed":167,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}}},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":["####3.1 Causal Model"],"metadata":{"id":"IOSj6PvCEdto"}},{"cell_type":"code","source":["all_data = []\n","punct_results = []\n","for sentence in punct_sentences:\n","    df = calculate_surprisal_causal(sentence)\n","    df['sentence'] = sentence\n","    print(df)\n","    print(\"*\" * 50)\n","    punct_results.append(df)\n","\n","    # Extract data and append to all_data list\n","    for _, row in df.iterrows():\n","        all_data.append([row['token'], row['surprisal'], sentence])\n","\n","\n","punct_df = pd.DataFrame(all_data, columns=['Token', 'Surprisal', 'Sentence'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YQg1eIGdIn9P","executionInfo":{"status":"ok","timestamp":1739385986600,"user_tz":300,"elapsed":2311,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}},"outputId":"6a3162bd-8674-4753-a36f-1c23e4f93237"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["     token  surprisal                                     sentence\n","0      The  12.220047  The man hunted the deer ran into the woods.\n","1      man   6.014305  The man hunted the deer ran into the woods.\n","2   hunted  11.088654  The man hunted the deer ran into the woods.\n","3      the   3.862102  The man hunted the deer ran into the woods.\n","4     deer   6.818150  The man hunted the deer ran into the woods.\n","5      ran  11.307253  The man hunted the deer ran into the woods.\n","6     into   2.734016  The man hunted the deer ran into the woods.\n","7      the   0.662579  The man hunted the deer ran into the woods.\n","8    woods   0.593332  The man hunted the deer ran into the woods.\n","9        .   2.190197  The man hunted the deer ran into the woods.\n","**************************************************\n","      token  surprisal                                      sentence\n","0       The  12.220047  The man hunted, the deer ran into the woods.\n","1       man   6.014305  The man hunted, the deer ran into the woods.\n","2    hunted  11.088654  The man hunted, the deer ran into the woods.\n","3         ,   3.797077  The man hunted, the deer ran into the woods.\n","4       the   2.216937  The man hunted, the deer ran into the woods.\n","5      deer   7.383633  The man hunted, the deer ran into the woods.\n","6       ran   4.075305  The man hunted, the deer ran into the woods.\n","7      into   5.538956  The man hunted, the deer ran into the woods.\n","8       the   0.388672  The man hunted, the deer ran into the woods.\n","9     woods   0.791360  The man hunted, the deer ran into the woods.\n","10        .   1.218499  The man hunted, the deer ran into the woods.\n","**************************************************\n"]}]},{"cell_type":"markdown","source":["####3.2 Masked Model"],"metadata":{"id":"kB51qCDlFWvl"}},{"cell_type":"code","source":["all_data = []\n","punct_results = []\n","for sentence in punct_sentences:\n","    df = calculate_surprisal_masked(sentence)\n","    df['sentence'] = sentence\n","    print(df)\n","    print(\"*\" * 50)\n","    punct_results.append(df)\n","\n","    # Extract data and append to all_data list\n","    for _, row in df.iterrows():\n","        all_data.append([row['token'], row['surprisal'], sentence])\n","\n","\n","punct_df = pd.DataFrame(all_data, columns=['Token', 'Surprisal', 'Sentence'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wGSxvwhQFZzw","executionInfo":{"status":"ok","timestamp":1739385998428,"user_tz":300,"elapsed":612,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}},"outputId":"b1200a95-3bfb-441b-8002-7d0ed85a77cc"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["     token  surprisal                                     sentence\n","0    [CLS]  14.793583  The man hunted the deer ran into the woods.\n","1      the   3.755283  The man hunted the deer ran into the woods.\n","2      man  20.501747  The man hunted the deer ran into the woods.\n","3   hunted  20.922817  The man hunted the deer ran into the woods.\n","4      the   8.661399  The man hunted the deer ran into the woods.\n","5     deer  18.019070  The man hunted the deer ran into the woods.\n","6      ran  16.659252  The man hunted the deer ran into the woods.\n","7     into  15.046646  The man hunted the deer ran into the woods.\n","8      the  12.250912  The man hunted the deer ran into the woods.\n","9    woods  13.686221  The man hunted the deer ran into the woods.\n","10       .  14.886926  The man hunted the deer ran into the woods.\n","11   [SEP]  31.439661  The man hunted the deer ran into the woods.\n","**************************************************\n","     token  surprisal                                      sentence\n","0    [CLS]  14.371651  The man hunted, the deer ran into the woods.\n","1      the   3.697596  The man hunted, the deer ran into the woods.\n","2      man  20.579172  The man hunted, the deer ran into the woods.\n","3   hunted  17.884607  The man hunted, the deer ran into the woods.\n","4        ,  13.599571  The man hunted, the deer ran into the woods.\n","5      the  21.501772  The man hunted, the deer ran into the woods.\n","6     deer  20.284107  The man hunted, the deer ran into the woods.\n","7      ran  16.228792  The man hunted, the deer ran into the woods.\n","8     into  15.585939  The man hunted, the deer ran into the woods.\n","9      the  11.317459  The man hunted, the deer ran into the woods.\n","10   woods  15.911333  The man hunted, the deer ran into the woods.\n","11       .  14.469553  The man hunted, the deer ran into the woods.\n","12   [SEP]  29.130955  The man hunted, the deer ran into the woods.\n","**************************************************\n"]}]},{"cell_type":"markdown","source":["####3.3 Permutation Model"],"metadata":{"id":"uzDX31evFekK"}},{"cell_type":"code","source":["punct_results = []\n","\n","for sentence in punct_sentences:\n","    tokens, surprisals = calculate_surprisal_permutation(sentence)\n","    df = pd.DataFrame({'token': tokens, 'surprisal': surprisals})\n","    df['sentence'] = sentence\n","    print(df)\n","    print(\"*\" * 50)\n","    punct_results.append(df)\n","\n","punct_df = pd.concat(punct_results, ignore_index=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hcu5TBjmFePE","executionInfo":{"status":"ok","timestamp":1739386084811,"user_tz":300,"elapsed":802,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}},"outputId":"edf2b934-34b2-4729-bcc7-fad8fd911378"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["      token  surprisal                                     sentence\n","0      ▁The        NaN  The man hunted the deer ran into the woods.\n","1      ▁man   9.656571  The man hunted the deer ran into the woods.\n","2   ▁hunted  15.746633  The man hunted the deer ran into the woods.\n","3      ▁the   5.620775  The man hunted the deer ran into the woods.\n","4     ▁deer  10.229841  The man hunted the deer ran into the woods.\n","5      ▁ran   7.447130  The man hunted the deer ran into the woods.\n","6     ▁into   6.161885  The man hunted the deer ran into the woods.\n","7      ▁the   6.603426  The man hunted the deer ran into the woods.\n","8    ▁woods  10.695705  The man hunted the deer ran into the woods.\n","9         .   3.174332  The man hunted the deer ran into the woods.\n","10    <sep>  15.432770  The man hunted the deer ran into the woods.\n","11    <cls>   9.393350  The man hunted the deer ran into the woods.\n","**************************************************\n","      token  surprisal                                      sentence\n","0      ▁The        NaN  The man hunted, the deer ran into the woods.\n","1      ▁man  15.173221  The man hunted, the deer ran into the woods.\n","2   ▁hunted  10.884846  The man hunted, the deer ran into the woods.\n","3         ,   5.000314  The man hunted, the deer ran into the woods.\n","4      ▁the   6.499137  The man hunted, the deer ran into the woods.\n","5     ▁deer  11.175428  The man hunted, the deer ran into the woods.\n","6      ▁ran   7.876666  The man hunted, the deer ran into the woods.\n","7     ▁into  10.174614  The man hunted, the deer ran into the woods.\n","8      ▁the   6.299298  The man hunted, the deer ran into the woods.\n","9    ▁woods  14.108970  The man hunted, the deer ran into the woods.\n","10        .   4.836990  The man hunted, the deer ran into the woods.\n","11    <sep>  14.393025  The man hunted, the deer ran into the woods.\n","12    <cls>  13.200272  The man hunted, the deer ran into the woods.\n","**************************************************\n"]}]},{"cell_type":"markdown","source":["### 4. First token"],"metadata":{"id":"7gO5OCN5-rep"}},{"cell_type":"code","source":["first_token_sentences = [\n","    \"The cat sat on the mat.\",\n","    \"A dog ran in the park.\",\n","    \"My friend likes ice cream.\",\n","    \"Some birds fly south in winter.\"\n","]"],"metadata":{"id":"52BSw8xRJuJ9","executionInfo":{"status":"ok","timestamp":1739385185903,"user_tz":300,"elapsed":3,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["####4.1 Causal Model"],"metadata":{"id":"v-rSICbEG0M4"}},{"cell_type":"code","source":["first_token_results = []\n","for sentence in first_token_sentences:\n","    df = calculate_surprisal_causal(sentence)\n","    first_token = df.iloc[1]\n","    first_token_results.append({\n","        'sentence': sentence,\n","        'first_token': first_token['token'],\n","        'surprisal': first_token['surprisal']\n","    })\n","\n","first_token_df = pd.DataFrame(first_token_results)\n","\n","print(first_token_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p96zAWDLKM-H","executionInfo":{"status":"ok","timestamp":1739391935010,"user_tz":300,"elapsed":6909,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}},"outputId":"3311562b-5c6c-4c00-b0ce-ce04e086bdea"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["                          sentence first_token  surprisal\n","0          The cat sat on the mat.         cat   9.115954\n","1           A dog ran in the park.         dog   8.534173\n","2       My friend likes ice cream.      friend   4.784204\n","3  Some birds fly south in winter.       birds   8.471863\n"]}]},{"cell_type":"markdown","source":["####4.2 Masked Model"],"metadata":{"id":"GNPncLUxG49S"}},{"cell_type":"code","source":["first_token_results = []\n","for sentence in punct_sentences:\n","    tokens, surprisals = calculate_surprisal_permutation(sentence)\n","    df = pd.DataFrame({'token': tokens, 'surprisal': surprisals})\n","    df['sentence'] = sentence\n","    first_token_results.append(df)\n","\n","first_token_df = pd.concat(first_token_results, ignore_index=True)\n","\n","\n","print(first_token_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8GNey02oMJuM","executionInfo":{"status":"ok","timestamp":1739392231878,"user_tz":300,"elapsed":595,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}},"outputId":"33433755-e22f-4837-8e14-24fc459c603f"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["      token  surprisal                                      sentence\n","0      ▁The        NaN   The man hunted the deer ran into the woods.\n","1      ▁man   9.656571   The man hunted the deer ran into the woods.\n","2   ▁hunted  15.746633   The man hunted the deer ran into the woods.\n","3      ▁the   5.620775   The man hunted the deer ran into the woods.\n","4     ▁deer  10.229841   The man hunted the deer ran into the woods.\n","5      ▁ran   7.447130   The man hunted the deer ran into the woods.\n","6     ▁into   6.161885   The man hunted the deer ran into the woods.\n","7      ▁the   6.603426   The man hunted the deer ran into the woods.\n","8    ▁woods  10.695705   The man hunted the deer ran into the woods.\n","9         .   3.174332   The man hunted the deer ran into the woods.\n","10    <sep>  15.432770   The man hunted the deer ran into the woods.\n","11    <cls>   9.393350   The man hunted the deer ran into the woods.\n","12     ▁The        NaN  The man hunted, the deer ran into the woods.\n","13     ▁man  15.173221  The man hunted, the deer ran into the woods.\n","14  ▁hunted  10.884846  The man hunted, the deer ran into the woods.\n","15        ,   5.000314  The man hunted, the deer ran into the woods.\n","16     ▁the   6.499137  The man hunted, the deer ran into the woods.\n","17    ▁deer  11.175428  The man hunted, the deer ran into the woods.\n","18     ▁ran   7.876666  The man hunted, the deer ran into the woods.\n","19    ▁into  10.174614  The man hunted, the deer ran into the woods.\n","20     ▁the   6.299298  The man hunted, the deer ran into the woods.\n","21   ▁woods  14.108970  The man hunted, the deer ran into the woods.\n","22        .   4.836990  The man hunted, the deer ran into the woods.\n","23    <sep>  14.393025  The man hunted, the deer ran into the woods.\n","24    <cls>  13.200272  The man hunted, the deer ran into the woods.\n"]}]},{"cell_type":"markdown","source":["####4.3 Permutation Model"],"metadata":{"id":"RsF--iriHBmu"}},{"cell_type":"code","source":["punct_results = []\n","\n","for sentence in punct_sentences:\n","    tokens, surprisals = calculate_surprisal_permutation(sentence)\n","    df = pd.DataFrame({'token': tokens, 'surprisal': surprisals})\n","    df['sentence'] = sentence\n","    punct_results.append(df)\n","\n","punct_df = pd.concat(punct_results, ignore_index=True)\n","\n","\n","print(\"\\nPunctuation analysis results:\")\n","for sentence in punct_sentences:\n","    print(f\"\\nSentence: {sentence}\")\n","    sentence_data = punct_df[punct_df['sentence'] == sentence]\n","    for _, row in sentence_data.iterrows():\n","        if row['surprisal'] is None:\n","            print(f\"Token: {row['token']}, Surprisal: N/A\")\n","        else:\n","            print(f\"Token: {row['token']}, Surprisal: {row['surprisal']:.3f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m2uBDTx_HDFf","executionInfo":{"status":"ok","timestamp":1739385193663,"user_tz":300,"elapsed":932,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}},"outputId":"75c628dd-e4b4-46a2-ea55-1f3ac4bfc0ba"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Punctuation analysis results:\n","\n","Sentence: I ate a pizza yesterday.\n","Token: ▁I, Surprisal: nan\n","Token: ▁, Surprisal: 6.196\n","Token: ate, Surprisal: 8.890\n","Token: ▁a, Surprisal: 8.804\n","Token: ▁pizza, Surprisal: 11.336\n","Token: ▁yesterday, Surprisal: 4.622\n","Token: ., Surprisal: 7.837\n","Token: <sep>, Surprisal: 15.849\n","Token: <cls>, Surprisal: 14.513\n","\n","Sentence: I ate a pizza, yesterday.\n","Token: ▁I, Surprisal: nan\n","Token: ▁, Surprisal: 7.341\n","Token: ate, Surprisal: 12.163\n","Token: ▁a, Surprisal: 13.233\n","Token: ▁pizza, Surprisal: 10.901\n","Token: ,, Surprisal: 5.119\n","Token: ▁yesterday, Surprisal: 3.473\n","Token: ., Surprisal: 10.349\n","Token: <sep>, Surprisal: 16.040\n","Token: <cls>, Surprisal: 14.460\n","\n","Sentence: I ate a pizza yesterday; I enjoyed every bite.\n","Token: ▁I, Surprisal: nan\n","Token: ▁, Surprisal: 7.920\n","Token: ate, Surprisal: 7.398\n","Token: ▁a, Surprisal: 8.778\n","Token: ▁pizza, Surprisal: 8.896\n","Token: ▁yesterday, Surprisal: 5.045\n","Token: ;, Surprisal: 12.790\n","Token: ▁I, Surprisal: 5.336\n","Token: ▁enjoyed, Surprisal: 11.743\n","Token: ▁every, Surprisal: 12.157\n","Token: ▁bite, Surprisal: 9.840\n","Token: ., Surprisal: 6.164\n","Token: <sep>, Surprisal: 15.888\n","Token: <cls>, Surprisal: 19.794\n","\n","Sentence: I ate an apple? No way!\n","Token: ▁I, Surprisal: nan\n","Token: ▁, Surprisal: 8.227\n","Token: ate, Surprisal: 9.550\n","Token: ▁an, Surprisal: 11.792\n","Token: ▁apple, Surprisal: 13.341\n","Token: ?, Surprisal: 6.683\n","Token: ▁No, Surprisal: 10.174\n","Token: ▁way, Surprisal: 11.144\n","Token: !, Surprisal: 6.253\n","Token: <sep>, Surprisal: 21.238\n","Token: <cls>, Surprisal: 9.681\n"]}]},{"cell_type":"markdown","source":["### 5. Different Languages"],"metadata":{"id":"_pKfsXgD-yvN"}},{"cell_type":"code","source":["model_name = \"facebook/xglm-564M\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","model.eval()\n","\n","def compute_surprisal_multilingual(sentence):\n","    # Tokenize the sentence and get input IDs\n","    input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n","\n","    with torch.no_grad():\n","        # Run the model; providing labels lets the model compute the logits.\n","        outputs = model(input_ids, labels=input_ids)\n","    logits = outputs.logits\n","\n","    # Shift logits and labels so that each prediction is aligned with the correct token.\n","    shifted_logits = logits[:, :-1, :]\n","    shifted_labels = input_ids[:, 1:]\n","\n","    # Convert logits to log probabilities.\n","    log_probs = torch.log_softmax(shifted_logits, dim=-1)\n","\n","    # For each token position (except the first), get the log probability of the correct token.\n","    token_log_probs = log_probs.gather(2, shifted_labels.unsqueeze(-1)).squeeze(-1)\n","\n","    # Compute surprisal as the negative log probability.\n","    computed_surprisals = (-token_log_probs.squeeze()).tolist()\n","\n","    # Convert the full input_ids to tokens.\n","    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n","\n","    # Prepend a placeholder (None) for the first token, which has no computed surprisal.\n","    full_surprisals = [None] + computed_surprisals\n","\n","    return tokens, full_surprisals\n","\n","\n","sentences = {\n","    'spanish_1': \"Comí una pizza ayer.\",                     # I ate a pizza yesterday.\n","    'spanish_2': \"Me comí una pizza deliciosa ayer.\",         # I ate a delicious pizza yesterday.\n","    'french_1': \"J'ai mangé une pizza hier.\",                 # I ate a pizza yesterday.\n","    'french_2': \"J'ai savouré une pizza délicieuse hier.\"      # I savored a delicious pizza yesterday.\n","}\n","\n","# Process each sentence and build a DataFrame with the token surprisal values.\n","results = []\n","for key, sentence in sentences.items():\n","    tokens, surprisals = compute_surprisal_multilingual(sentence)\n","    df = pd.DataFrame({'token': tokens, 'surprisal': surprisals})\n","    df['sentence'] = sentence\n","    # Use the language (spanish or french) based on the key.\n","    df['language'] = key.split('_')[0]\n","    results.append(df)\n","\n","# Concatenate all individual DataFrames into one final DataFrame.\n","final_df = pd.concat(results, ignore_index=True)\n","\n","# Optionally, print token-by-token surprisal for each sentence.\n","print(\"\\nToken-by-token surprisal values per sentence:\")\n","for key, sentence in sentences.items():\n","    print(f\"\\nSentence: {sentence}\")\n","    sentence_data = final_df[final_df['sentence'] == sentence]\n","    for _, row in sentence_data.iterrows():\n","        surprisal_str = \"N/A\" if row['surprisal'] is None else f\"{row['surprisal']:.3f}\"\n","        print(f\"Token: {row['token']}, Surprisal: {surprisal_str}\")\n"],"metadata":{"id":"Z67MsuzO-vEK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739385214957,"user_tz":300,"elapsed":21299,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}},"outputId":"a166ef3f-3edf-4803-bf3c-c8d2013255b0"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Token-by-token surprisal values per sentence:\n","\n","Sentence: Comí una pizza ayer.\n","Token: </s>, Surprisal: nan\n","Token: ▁Com, Surprisal: 7.666\n","Token: í, Surprisal: 7.407\n","Token: ▁una, Surprisal: 3.117\n","Token: ▁pizza, Surprisal: 4.189\n","Token: ▁ayer, Surprisal: 11.494\n","Token: ., Surprisal: 2.453\n","\n","Sentence: Me comí una pizza deliciosa ayer.\n","Token: </s>, Surprisal: nan\n","Token: ▁Me, Surprisal: 7.303\n","Token: ▁com, Surprisal: 8.683\n","Token: í, Surprisal: 6.469\n","Token: ▁una, Surprisal: 9.090\n","Token: ▁pizza, Surprisal: 3.698\n","Token: ▁delicios, Surprisal: 3.996\n","Token: a, Surprisal: 0.005\n","Token: ▁ayer, Surprisal: 9.851\n","Token: ., Surprisal: 1.803\n","\n","Sentence: J'ai mangé une pizza hier.\n","Token: </s>, Surprisal: nan\n","Token: ▁J, Surprisal: 6.897\n","Token: ', Surprisal: 2.009\n","Token: ai, Surprisal: 0.697\n","Token: ▁mang, Surprisal: 7.612\n","Token: é, Surprisal: 0.239\n","Token: ▁une, Surprisal: 2.680\n","Token: ▁pizza, Surprisal: 4.618\n","Token: ▁hier, Surprisal: 7.304\n","Token: ., Surprisal: 3.227\n","\n","Sentence: J'ai savouré une pizza délicieuse hier.\n","Token: </s>, Surprisal: nan\n","Token: ▁J, Surprisal: 6.897\n","Token: ', Surprisal: 2.009\n","Token: ai, Surprisal: 0.697\n","Token: ▁savo, Surprisal: 10.670\n","Token: uré, Surprisal: 0.086\n","Token: ▁une, Surprisal: 3.949\n","Token: ▁pizza, Surprisal: 6.888\n","Token: ▁déli, Surprisal: 6.013\n","Token: cieuse, Surprisal: 0.002\n","Token: ▁hier, Surprisal: 8.576\n","Token: ., Surprisal: 2.688\n"]}]}]}