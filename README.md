# Evaluating Syntactic Comprehension of Large Language Models on Garden-Path Sentences  

## **Overview**  
This project investigates the **syntactic comprehension** capabilities of **Large Language Models (LLMs)** by analyzing how they process **garden-path sentences**â€”a type of sentence that initially leads readers to an incorrect interpretation. The goal is to compare **computational parsing behaviors of LLMs** against **human cognitive patterns** to better understand model limitations in syntactic disambiguation.  

## **Motivation**  
While LLMs have shown impressive performance in natural language understanding, their ability to **resolve syntactic ambiguity** remains an open question. Human readers rely on **incremental parsing strategies**, guided by **cognitive heuristics**. However, LLMs process language differently, often relying on statistical associations rather than deep structural understanding. By studying their responses to **garden-path sentences**, we can assess:  
- How well LLMs recover from misinterpretations.  
- Whether their parsing behavior aligns with human cognitive processing.  
- The effectiveness of **fine-tuning** or **prompting strategies** in improving syntactic comprehension.  
