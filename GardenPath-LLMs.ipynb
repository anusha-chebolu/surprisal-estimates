{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMSScomjJqsmH/J3nS1FbkB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# GPT-2 and Flan-T5"],"metadata":{"id":"4xdac477KsFb"}},{"cell_type":"code","source":["import torch\n","from transformers import (\n","    AutoModelForQuestionAnswering,\n","    AutoModelForCausalLM,\n","    AutoModelForSeq2SeqLM,\n","    AutoTokenizer\n",")\n","\n","def load_models_and_tokenizers():\n","    \"\"\"Load models and tokenizers with proper error handling.\"\"\"\n","    tokenizers = {}\n","    loaded_models = {}\n","    model_configs = {\n","        \"GPT-2\": (\"gpt2\", AutoModelForCausalLM),\n","        \"Flan-T5\": (\"google/flan-t5-large\", AutoModelForSeq2SeqLM),\n","        \"XLNet\": (\"xlnet-base-cased\", AutoModelForQuestionAnswering),\n","        \"RoBERTa\": (\"deepset/roberta-base-squad2\", AutoModelForQuestionAnswering)\n","    }\n","\n","    for model_name, (model_path, model_class) in model_configs.items():\n","        try:\n","            tokenizer = AutoTokenizer.from_pretrained(model_path)\n","            # Set padding token for GPT-2\n","            if model_name == \"GPT-2\":\n","                tokenizer.pad_token = tokenizer.eos_token\n","\n","            tokenizers[model_name] = tokenizer\n","            loaded_models[model_name] = model_class.from_pretrained(model_path)\n","            print(f\"Successfully loaded {model_name}\")\n","        except Exception as e:\n","            print(f\"Error loading {model_name}: {str(e)}\")\n","\n","    return tokenizers, loaded_models\n","\n","def query_model(model_name, sentence, question, tokenizer, model):\n","    \"\"\"Query the model with improved error handling and response formatting.\"\"\"\n","    try:\n","        if model_name == \"GPT-2\":\n","            input_text = f\"Read this sentence carefully: {sentence}\\nQuestion: {question}\\nAnswer (Yes or No):\"\n","            inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n","            outputs = model.generate(\n","                **inputs,\n","                max_length=100,\n","                num_return_sequences=1,\n","                temperature=0.7,\n","                pad_token_id=tokenizer.eos_token_id\n","            )\n","            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            # Extract just the answer part\n","            answer = response.split(\"Answer (Yes or No):\")[-1].strip()\n","            return answer if answer else \"No clear answer\"\n","\n","        elif model_name == \"Flan-T5\":\n","            input_text = f\"Based on this context: {sentence} Answer this yes/no question: {question}\"\n","            inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n","            outputs = model.generate(\n","                **inputs,\n","                max_length=50,\n","                num_return_sequences=1,\n","                temperature=0.7\n","            )\n","            return tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","        elif model_name in [\"XLNet\", \"RoBERTa\"]:\n","            # For QA models, we need to handle the answer span extraction differently\n","            inputs = tokenizer(\n","                question,\n","                sentence,\n","                return_tensors=\"pt\",\n","                padding=True,\n","                truncation=True,\n","                max_length=512\n","            )\n","\n","            with torch.no_grad():\n","                outputs = model(**inputs)\n","\n","            # Get the most likely answer span\n","            start_logits = outputs.start_logits\n","            end_logits = outputs.end_logits\n","\n","            # Get top 5 most likely start and end positions\n","            top_starts = torch.topk(start_logits, 5)\n","            top_ends = torch.topk(end_logits, 5)\n","\n","            best_answer = \"No clear answer found\"\n","            best_score = float('-inf')\n","\n","            # Try different combinations of start and end positions\n","            for start_idx in top_starts.indices[0]:\n","                for end_idx in top_ends.indices[0]:\n","                    if start_idx <= end_idx:\n","                        answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx + 1]\n","                        answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n","                        score = start_logits[0][start_idx] + end_logits[0][end_idx]\n","\n","                        if score > best_score and len(answer.strip()) > 0:\n","                            best_answer = answer\n","                            best_score = score\n","\n","            return best_answer.strip()\n","\n","    except Exception as e:\n","        return f\"Error: {str(e)}\"\n","\n","def analyze_garden_path_sentence(sentence, question):\n","    \"\"\"Analyze a garden-path sentence with progressive chunks and formatted output.\"\"\"\n","    tokenizers, models = load_models_and_tokenizers()\n","\n","    # Create meaningful chunks based on phrases\n","    words = sentence.split()\n","    chunks = []\n","    current_chunk = []\n","\n","    for word in words:\n","        current_chunk.append(word)\n","        if word in ['.', ',', ';', '?', '!'] or word.endswith(('.', ',', ';', '?', '!')):\n","            chunks.append(' '.join(current_chunk))\n","            current_chunk = []\n","        elif len(current_chunk) >= 3:  # Create chunks of meaningful phrases\n","            chunks.append(' '.join(current_chunk))\n","            current_chunk = []\n","\n","    if current_chunk:  # Add any remaining words\n","        chunks.append(' '.join(current_chunk))\n","\n","    # Remove duplicates while preserving order\n","    chunks = list(dict.fromkeys(chunks))\n","\n","    results = {}\n","    for model_name in models:\n","        results[model_name] = []\n","        accumulated_text = \"\"\n","        for chunk in chunks:\n","            accumulated_text = (accumulated_text + \" \" + chunk).strip()\n","            response = query_model(\n","                model_name,\n","                accumulated_text,\n","                question,\n","                tokenizers[model_name],\n","                models[model_name]\n","            )\n","            results[model_name].append({\n","                'chunk': accumulated_text,\n","                'response': response\n","            })\n","\n","    # Print formatted results\n","    print(f\"\\nAnalyzing Garden Path Sentence:\")\n","    print(f\"Full sentence: '{sentence}'\")\n","    print(f\"Question: '{question}'\\n\")\n","\n","    for model_name, responses in results.items():\n","        print(f\"\\n{model_name} Progressive Analysis:\")\n","        print(\"-\" * 60)\n","        for i, resp in enumerate(responses, 1):\n","            print(f\"\\nChunk {i}: '{resp['chunk']}'\")\n","            print(f\"Response: {resp['response']}\")\n","        print(\"-\" * 60)\n","\n","    return results\n","\n","# Example usage\n","garden_path_sentence = \"While the man hunted the deer ran through the woods.\"\n","question = \"Did the man hunt the deer?\"\n","results = analyze_garden_path_sentence(garden_path_sentence, question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oNmJ1KbSZzsk","executionInfo":{"status":"ok","timestamp":1739419051347,"user_tz":300,"elapsed":90479,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}},"outputId":"a0f19228-0a66-45e4-cd9c-c47add8c1c03"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully loaded GPT-2\n","Successfully loaded Flan-T5\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of XLNetForQuestionAnsweringSimple were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Successfully loaded XLNet\n","Successfully loaded RoBERTa\n","\n","Analyzing Garden Path Sentence:\n","Full sentence: 'While the man hunted the deer ran through the woods.'\n","Question: 'Did the man hunt the deer?'\n","\n","\n","GPT-2 Progressive Analysis:\n","------------------------------------------------------------\n","\n","Chunk 1: 'While the man'\n","Response: No.\n","Question: Did the man hunt the deer?\n","Answer (No): No.\n","Question: Did the man hunt the deer?\n","Answer (No): No.\n","Question: Did the man hunt the deer?\n","Answer (No): No.\n","Question: Did the man hunt the deer?\n","Answer (No): No.\n","Question: Did the\n","\n","Chunk 2: 'While the man hunted the deer'\n","Response: No.\n","Question: Did the man hunt the deer?\n","Answer (No or No): No.\n","Question: Did the man hunt the deer?\n","Answer (No or No): No.\n","Question: Did the man hunt the deer?\n","Answer (No or No): No.\n","Question: Did the man hunt the deer?\n","Answer (\n","\n","Chunk 3: 'While the man hunted the deer ran through the'\n","Response: No.\n","Question: Did the man hunt the deer?\n","Answer (No): No.\n","Question: Did the man hunt the deer?\n","Answer (No): No.\n","Question: Did the man hunt the deer?\n","Answer (No): No.\n","Question: Did the man hunt the deer?\n","Answer (No): No\n","\n","Chunk 4: 'While the man hunted the deer ran through the woods.'\n","Response: No.\n","Question: Did the man hunt the deer?\n","Answer (No or No): No.\n","Question: Did the man hunt the deer?\n","Answer (No or No): No.\n","Question: Did the man hunt the deer?\n","Answer (No or No): No.\n","Question: Did the man hunt the\n","------------------------------------------------------------\n","\n","Flan-T5 Progressive Analysis:\n","------------------------------------------------------------\n","\n","Chunk 1: 'While the man'\n","Response: no\n","\n","Chunk 2: 'While the man hunted the deer'\n","Response: yes\n","\n","Chunk 3: 'While the man hunted the deer ran through the'\n","Response: yes\n","\n","Chunk 4: 'While the man hunted the deer ran through the woods.'\n","Response: yes\n","------------------------------------------------------------\n","\n","XLNet Progressive Analysis:\n","------------------------------------------------------------\n","\n","Chunk 1: 'While the man'\n","Response: hunt the deer? While the man\n","\n","Chunk 2: 'While the man hunted the deer'\n","Response: the deer? While the man hunted the deer\n","\n","Chunk 3: 'While the man hunted the deer ran through the'\n","Response: hunted the deer ran\n","\n","Chunk 4: 'While the man hunted the deer ran through the woods.'\n","Response: man hunted the deer ran\n","------------------------------------------------------------\n","\n","RoBERTa Progressive Analysis:\n","------------------------------------------------------------\n","\n","Chunk 1: 'While the man'\n","Response: Did the man hunt the deer?While the man\n","\n","Chunk 2: 'While the man hunted the deer'\n","Response: Did the man hunt the deer?\n","\n","Chunk 3: 'While the man hunted the deer ran through the'\n","Response: While the man hunted\n","\n","Chunk 4: 'While the man hunted the deer ran through the woods.'\n","Response: While the man hunted the deer ran through the woods\n","------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import (\n","    AutoModelForQuestionAnswering,\n","    AutoModelForCausalLM,\n","    AutoModelForSeq2SeqLM,\n","    AutoTokenizer\n",")\n","\n","# Load tokenizers\n","tokenizers = {\n","    \"GPT-2\": AutoTokenizer.from_pretrained(\"gpt2\"),\n","    \"Flan-T5\": AutoTokenizer.from_pretrained(\"google/flan-t5-large\"),\n","    \"XLNet\": AutoTokenizer.from_pretrained(\"xlnet-large-cased\"),  # ✅ Fine-tuned XLNet for QA\n","    \"RoBERTa\": AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n","}\n","\n","# Load models\n","loaded_models = {\n","    \"GPT-2\": AutoModelForCausalLM.from_pretrained(\"gpt2\"),  # ✅ Supports `.generate()`\n","    \"Flan-T5\": AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\"),  # ✅ Supports `.generate()`\n","    \"XLNet\": AutoModelForQuestionAnswering.from_pretrained(\"xlnet-large-cased\"),  # ✅ XLNet for QA\n","    \"RoBERTa\": AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")\n","}\n","\n","def query_model(model_name, sentence, question):\n","    \"\"\"Query a model with a sentence and comprehension question.\"\"\"\n","    tokenizer = tokenizers.get(model_name)\n","    model = loaded_models.get(model_name)\n","\n","    if model_name == \"GPT-2\":\n","        input_text = f\"{sentence}\\nQuestion: {question}\\nAnswer: \"\n","        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True)\n","        outputs = model.generate(**inputs, max_length=50, pad_token_id=tokenizer.eos_token_id)  # ✅ Fix padding issue\n","        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    elif model_name == \"Flan-T5\":\n","        input_text = f\"question: {question}  context: {sentence}\"\n","        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True)\n","        outputs = model.generate(**inputs, max_length=50)  # ✅ Ensure proper formatting\n","        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    elif model_name in [\"XLNet\", \"RoBERTa\"]:  # ✅ Now using proper QA models\n","        inputs = tokenizer(sentence, question, return_tensors=\"pt\", truncation=True)\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","\n","        start_logits = outputs.start_logits\n","        end_logits = outputs.end_logits\n","\n","        start_idx = torch.argmax(start_logits)\n","        end_idx = torch.argmax(end_logits) + 1  # End index is inclusive\n","\n","        answer = tokenizer.convert_tokens_to_string(\n","            tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_idx:end_idx])\n","        )\n","        return answer.strip()\n","\n","    return \"Model not supported\"\n","\n","\n","def process_sentence(sentence, question):\n","    \"\"\"Process a garden-path sentence incrementally.\"\"\"\n","    sentence_chunks = [\n","        \"While the man hunted\",\n","        \"the deer\",\n","        \"ran\",\n","        \"through the woods.\"\n","    ]\n","    responses = {}\n","\n","    for model_name in loaded_models:\n","        model_responses = []\n","        context = \"\"\n","        for chunk in sentence_chunks:\n","            context += chunk + \" \"\n","            response = query_model(model_name, context, question)\n","            model_responses.append(response)\n","        responses[model_name] = model_responses\n","\n","    return responses\n","\n","\n","# Example Usage\n","garden_path_sentence = \"While the man hunted the deer ran through the woods.\"\n","question = \"Did the man hunt the deer?\"\n","\n","responses = process_sentence(garden_path_sentence, question)\n","\n","# Print results\n","print(f\"Sentence: {garden_path_sentence}\")\n","for model_name, model_responses in responses.items():\n","    print(f\"\\n{model_name}:\")\n","    for i, response in enumerate(model_responses):\n","        print(f\"  Chunk {i+1}: {response}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TmFTGi9Acdzn","executionInfo":{"status":"ok","timestamp":1739419197439,"user_tz":300,"elapsed":52480,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}},"outputId":"8efb732a-2007-439b-d4f1-a480769c5282"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of XLNetForQuestionAnsweringSimple were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]},{"output_type":"stream","name":"stdout","text":["Sentence: While the man hunted the deer ran through the woods.\n","\n","GPT-2:\n","  Chunk 1: While the man hunted \n","Question: Did the man hunt the deer?\n","Answer:  Yes.  The man hunted the deer.  The man hunted the deer.  The man hunted the deer.  The man\n","  Chunk 2: While the man hunted the deer \n","Question: Did the man hunt the deer?\n","Answer:  Yes.  The man hunted the deer.  The man hunted the deer.  The man hunted the deer.  \n","  Chunk 3: While the man hunted the deer ran \n","Question: Did the man hunt the deer?\n","Answer:  Yes.  The man hunted the deer.  The man hunted the deer.  The man hunted the deer. \n","  Chunk 4: While the man hunted the deer ran through the woods. \n","Question: Did the man hunt the deer?\n","Answer:  No.  The man hunted the deer. \n","Question: Did the man hunt the deer?\n","Answer\n","\n","Flan-T5:\n","  Chunk 1: deer , he was unable to kill one.\n","  Chunk 2: he was unable to kill it.\n","  Chunk 3: no\n","  Chunk 4: yes\n","\n","XLNet:\n","  Chunk 1: hunted\n","  Chunk 2: the\n","  Chunk 3: the\n","  Chunk 4: hunted\n","\n","RoBERTa:\n","  Chunk 1: <s>\n","  Chunk 2: <s>\n","  Chunk 3: <s>\n","  Chunk 4: <s>\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import (\n","    AutoModelForQuestionAnswering,\n","    AutoModelForCausalLM,\n","    AutoModelForSeq2SeqLM,\n","    AutoTokenizer\n",")\n","\n","def load_models_and_tokenizers():\n","    \"\"\"Load models and tokenizers with proper error handling.\"\"\"\n","    tokenizers = {}\n","    loaded_models = {}\n","    model_configs = {\n","        \"GPT-2\": (\"gpt2\", AutoModelForCausalLM),\n","        \"Flan-T5\": (\"google/flan-t5-large\", AutoModelForSeq2SeqLM),\n","        \"XLNet\": (\"xlnet-base-cased\", AutoModelForQuestionAnswering),\n","        \"RoBERTa\": (\"deepset/roberta-base-squad2\", AutoModelForQuestionAnswering)\n","    }\n","\n","    for model_name, (model_path, model_class) in model_configs.items():\n","        try:\n","            tokenizer = AutoTokenizer.from_pretrained(model_path)\n","            # Set padding token for GPT-2\n","            if model_name == \"GPT-2\":\n","                tokenizer.pad_token = tokenizer.eos_token\n","\n","            tokenizers[model_name] = tokenizer\n","            loaded_models[model_name] = model_class.from_pretrained(model_path)\n","            print(f\"Successfully loaded {model_name}\")\n","        except Exception as e:\n","            print(f\"Error loading {model_name}: {str(e)}\")\n","\n","    return tokenizers, loaded_models\n","\n","def query_model(model_name, sentence, question, tokenizer, model):\n","    \"\"\"Query the model with improved error handling and response formatting.\"\"\"\n","    try:\n","        if model_name == \"GPT-2\":\n","            input_text = f\"Read this sentence carefully: {sentence}\\nQuestion: {question}\\nAnswer (Yes or No):\"\n","            inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n","            outputs = model.generate(\n","                **inputs,\n","                max_length=100,\n","                num_return_sequences=1,\n","                temperature=0.7,\n","                pad_token_id=tokenizer.eos_token_id\n","            )\n","            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            # Extract just the answer part\n","            answer = response.split(\"Answer (Yes or No):\")[-1].strip()\n","            return answer if answer else \"No clear answer\"\n","\n","        elif model_name == \"Flan-T5\":\n","            input_text = f\"Based on this context: {sentence} Answer this yes/no question: {question}\"\n","            inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n","            outputs = model.generate(\n","                **inputs,\n","                max_length=50,\n","                num_return_sequences=1,\n","                temperature=0.7\n","            )\n","            return tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","        elif model_name in [\"XLNet\", \"RoBERTa\"]:\n","            # For QA models, we need to handle the answer span extraction differently\n","            inputs = tokenizer(\n","                question,\n","                sentence,\n","                return_tensors=\"pt\",\n","                padding=True,\n","                truncation=True,\n","                max_length=512\n","            )\n","\n","            with torch.no_grad():\n","                outputs = model(**inputs)\n","\n","            # Get the most likely answer span\n","            start_logits = outputs.start_logits\n","            end_logits = outputs.end_logits\n","\n","            # Get top 5 most likely start and end positions\n","            top_starts = torch.topk(start_logits, 5)\n","            top_ends = torch.topk(end_logits, 5)\n","\n","            best_answer = \"No clear answer found\"\n","            best_score = float('-inf')\n","\n","            # Try different combinations of start and end positions\n","            for start_idx in top_starts.indices[0]:\n","                for end_idx in top_ends.indices[0]:\n","                    if start_idx <= end_idx:\n","                        answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx + 1]\n","                        answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n","                        score = start_logits[0][start_idx] + end_logits[0][end_idx]\n","\n","                        if score > best_score and len(answer.strip()) > 0:\n","                            best_answer = answer\n","                            best_score = score\n","\n","            return best_answer.strip()\n","\n","    except Exception as e:\n","        return f\"Error: {str(e)}\"\n","\n","def analyze_garden_path_sentence(sentence, question, sentence_chunks):\n","    \"\"\"Analyze a garden-path sentence using predefined chunks.\"\"\"\n","    tokenizers, models = load_models_and_tokenizers()\n","\n","    results = {}\n","    for model_name in models:\n","        results[model_name] = []\n","        accumulated_text = \"\"\n","        for chunk in sentence_chunks:\n","            accumulated_text = (accumulated_text + \" \" + chunk).strip()\n","            response = query_model(\n","                model_name,\n","                accumulated_text,\n","                question,\n","                tokenizers[model_name],\n","                models[model_name]\n","            )\n","            results[model_name].append({\n","                'chunk': accumulated_text,\n","                'response': response\n","            })\n","\n","    # Print formatted results\n","    print(f\"\\nAnalyzing Garden Path Sentence:\")\n","    print(f\"Full sentence: '{sentence}'\")\n","    print(f\"Question: '{question}'\\n\")\n","\n","    for model_name, responses in results.items():\n","        print(f\"\\n{model_name} Progressive Analysis:\")\n","        print(\"-\" * 60)\n","        for i, resp in enumerate(responses, 1):\n","            print(f\"\\nChunk {i}: '{resp['chunk']}'\")\n","            print(f\"Response: {resp['response']}\")\n","        print(\"-\" * 60)\n","\n","    return results\n","\n","# Example usage\n","garden_path_sentence = \"While the man hunted the deer ran through the woods.\"\n","sentence_chunks = [\n","    \"While the man hunted\",\n","    \"the deer\",\n","    \"ran\",\n","    \"through the woods.\"\n","]\n","question = \"Did the man hunt the deer?\"\n","results = analyze_garden_path_sentence(garden_path_sentence, question, sentence_chunks)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AIwywzdCddHT","executionInfo":{"status":"ok","timestamp":1739419308158,"user_tz":300,"elapsed":65246,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}},"outputId":"4000061c-488a-4485-a3c8-e0b6b0f3b5ab"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully loaded GPT-2\n","Successfully loaded Flan-T5\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of XLNetForQuestionAnsweringSimple were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Successfully loaded XLNet\n","Successfully loaded RoBERTa\n","\n","Analyzing Garden Path Sentence:\n","Full sentence: 'While the man hunted the deer ran through the woods.'\n","Question: 'Did the man hunt the deer?'\n","\n","\n","GPT-2 Progressive Analysis:\n","------------------------------------------------------------\n","\n","Chunk 1: 'While the man hunted'\n","Response: No.\n","Question: Did the man hunt the deer?\n","Answer (No): No.\n","Question: Did the man hunt the deer?\n","Answer (No): No.\n","Question: Did the man hunt the deer?\n","Answer (No): No.\n","Question: Did the man hunt the deer?\n","Answer (No): No.\n","Question: Did\n","\n","Chunk 2: 'While the man hunted the deer'\n","Response: No.\n","Question: Did the man hunt the deer?\n","Answer (No or No): No.\n","Question: Did the man hunt the deer?\n","Answer (No or No): No.\n","Question: Did the man hunt the deer?\n","Answer (No or No): No.\n","Question: Did the man hunt the deer?\n","Answer (\n","\n","Chunk 3: 'While the man hunted the deer ran'\n","Response: No.\n","Question: Did the man hunt the deer?\n","Answer (No or No): No.\n","Question: Did the man hunt the deer?\n","Answer (No or No): No.\n","Question: Did the man hunt the deer?\n","Answer (No or No): No.\n","Question: Did the man hunt the deer?\n","Answer\n","\n","Chunk 4: 'While the man hunted the deer ran through the woods.'\n","Response: No.\n","Question: Did the man hunt the deer?\n","Answer (No or No): No.\n","Question: Did the man hunt the deer?\n","Answer (No or No): No.\n","Question: Did the man hunt the deer?\n","Answer (No or No): No.\n","Question: Did the man hunt the\n","------------------------------------------------------------\n","\n","Flan-T5 Progressive Analysis:\n","------------------------------------------------------------\n","\n","Chunk 1: 'While the man hunted'\n","Response: yes\n","\n","Chunk 2: 'While the man hunted the deer'\n","Response: yes\n","\n","Chunk 3: 'While the man hunted the deer ran'\n","Response: yes\n","\n","Chunk 4: 'While the man hunted the deer ran through the woods.'\n","Response: yes\n","------------------------------------------------------------\n","\n","XLNet Progressive Analysis:\n","------------------------------------------------------------\n","\n","Chunk 1: 'While the man hunted'\n","Response: the\n","\n","Chunk 2: 'While the man hunted the deer'\n","Response: hunt the\n","\n","Chunk 3: 'While the man hunted the deer ran'\n","Response: deer? While the man\n","\n","Chunk 4: 'While the man hunted the deer ran through the woods.'\n","Response: hunted the deer ran through the\n","------------------------------------------------------------\n","\n","RoBERTa Progressive Analysis:\n","------------------------------------------------------------\n","\n","Chunk 1: 'While the man hunted'\n","Response: Did the man hunt the deer?While the man hunted\n","\n","Chunk 2: 'While the man hunted the deer'\n","Response: Did the man hunt the deer?\n","\n","Chunk 3: 'While the man hunted the deer ran'\n","Response: ran\n","\n","Chunk 4: 'While the man hunted the deer ran through the woods.'\n","Response: While the man hunted the deer ran through the woods\n","------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import (\n","    AutoModelForQuestionAnswering,\n","    AutoModelForCausalLM,\n","    AutoModelForSeq2SeqLM,\n","    AutoTokenizer\n",")\n","\n","def load_models_and_tokenizers():\n","    \"\"\"Load models and tokenizers with proper error handling.\"\"\"\n","    tokenizers = {}\n","    loaded_models = {}\n","    model_configs = {\n","        \"GPT-2\": (\"gpt2\", AutoModelForCausalLM),\n","        \"Flan-T5\": (\"google/flan-t5-large\", AutoModelForSeq2SeqLM),\n","        \"XLNet\": (\"xlnet-base-cased\", AutoModelForQuestionAnswering),\n","        \"RoBERTa\": (\"deepset/roberta-base-squad2\", AutoModelForQuestionAnswering)\n","    }\n","\n","    for model_name, (model_path, model_class) in model_configs.items():\n","        tokenizer = AutoTokenizer.from_pretrained(model_path)\n","        if model_name == \"GPT-2\":\n","            tokenizer.pad_token = tokenizer.eos_token\n","\n","        tokenizers[model_name] = tokenizer\n","        loaded_models[model_name] = model_class.from_pretrained(model_path)\n","\n","    return tokenizers, loaded_models\n","\n","def query_model(model_name, sentence, question, tokenizer, model):\n","    \"\"\"Query the model with improved error handling and response formatting.\"\"\"\n","    if model_name == \"GPT-2\":\n","        input_text = f\"Read this sentence carefully: {sentence}\\nQuestion: {question}\\nAnswer: \"\n","        inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n","        outputs = model.generate(\n","            **inputs,\n","            max_new_tokens=10,\n","            temperature=0.5,\n","            repetition_penalty=1.5,\n","            pad_token_id=tokenizer.eos_token_id\n","        )\n","        return tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"Answer:\")[-1].strip()\n","\n","    elif model_name == \"Flan-T5\":\n","        input_text = f\"Based on this context: {sentence} Answer this yes/no question: {question}\\nAnswer: \"\n","        inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n","        outputs = model.generate(\n","            **inputs,\n","            max_new_tokens=5,\n","            do_sample=False\n","        )\n","        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    elif model_name in [\"XLNet\", \"RoBERTa\"]:\n","        inputs = tokenizer(question, sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","\n","        start_logits = outputs.start_logits\n","        end_logits = outputs.end_logits\n","\n","        # Get best answer span\n","        start_idx = torch.argmax(start_logits)\n","        end_idx = torch.argmax(end_logits) + 1\n","        answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx]\n","        answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n","\n","        return answer.strip() if answer else \"No clear answer\"\n","\n","def analyze_garden_path_sentence(sentence, question, sentence_chunks):\n","    \"\"\"Analyze a garden-path sentence using predefined chunks.\"\"\"\n","    tokenizers, models = load_models_and_tokenizers()\n","\n","    results = {}\n","    for model_name in models:\n","        results[model_name] = []\n","        accumulated_text = \"\"\n","        for chunk in sentence_chunks:\n","            accumulated_text = (accumulated_text + \" \" + chunk).strip()\n","            response = query_model(\n","                model_name,\n","                accumulated_text,\n","                question,\n","                tokenizers[model_name],\n","                models[model_name]\n","            )\n","            results[model_name].append({'chunk': accumulated_text, 'response': response})\n","\n","    for model_name, responses in results.items():\n","        print(f\"\\n{model_name} Progressive Analysis:\")\n","        for i, resp in enumerate(responses, 1):\n","            print(f\"\\nChunk {i}: '{resp['chunk']}'\")\n","            print(f\"Response: {resp['response']}\")\n","\n","    return results\n","\n","# Example Usage\n","garden_path_sentence = \"While the man hunted the deer ran through the woods.\"\n","sentence_chunks = [\n","    \"While the man hunted\",\n","    \"the deer\",\n","    \"ran\",\n","    \"through the woods.\"\n","]\n","question = \"Did the man hunt the deer?\"\n","results = analyze_garden_path_sentence(garden_path_sentence, question, sentence_chunks)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k2_Eyn4_fCpH","executionInfo":{"status":"ok","timestamp":1739420225607,"user_tz":300,"elapsed":36504,"user":{"displayName":"Srilalitha Lakshmi Anusha Chebolu","userId":"00938766134340442495"}},"outputId":"b99ffe16-d7ba-4571-d109-543c36caedb4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Some weights of XLNetForQuestionAnsweringSimple were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","GPT-2 Progressive Analysis:\n","\n","Chunk 1: 'While the man hunted'\n","Response: Yes. The hunter was not a member of\n","\n","Chunk 2: 'While the man hunted the deer'\n","Response: Yes. The hunter was not a member of\n","\n","Chunk 3: 'While the man hunted the deer ran'\n","Response: Yes. The hunter was not a member of\n","\n","Chunk 4: 'While the man hunted the deer ran through the woods.'\n","Response: Yes, he did so in a manner that\n","\n","Flan-T5 Progressive Analysis:\n","\n","Chunk 1: 'While the man hunted'\n","Response: no\n","\n","Chunk 2: 'While the man hunted the deer'\n","Response: Yes\n","\n","Chunk 3: 'While the man hunted the deer ran'\n","Response: yes\n","\n","Chunk 4: 'While the man hunted the deer ran through the woods.'\n","Response: Yes\n","\n","XLNet Progressive Analysis:\n","\n","Chunk 1: 'While the man hunted'\n","Response: While the man hunted\n","\n","Chunk 2: 'While the man hunted the deer'\n","Response: No clear answer\n","\n","Chunk 3: 'While the man hunted the deer ran'\n","Response: the man hunted the deer ran\n","\n","Chunk 4: 'While the man hunted the deer ran through the woods.'\n","Response: While the man hunted the deer ran\n","\n","RoBERTa Progressive Analysis:\n","\n","Chunk 1: 'While the man hunted'\n","Response: No clear answer\n","\n","Chunk 2: 'While the man hunted the deer'\n","Response: No clear answer\n","\n","Chunk 3: 'While the man hunted the deer ran'\n","Response: ran\n","\n","Chunk 4: 'While the man hunted the deer ran through the woods.'\n","Response: While the man hunted the deer ran through the woods\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"0TLzSkh540u1"},"execution_count":null,"outputs":[]}]}